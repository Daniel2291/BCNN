{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "-AuoQQEBUC_W",
        "eVb7NwIAa0mn"
      ],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Daniel2291/BCNN/blob/main/BCNN_Development.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-AuoQQEBUC_W"
      },
      "source": [
        "# **Introduction**\n",
        "\n",
        "In this lab, you will learn to use Google Colab hardware resources e.g. CPU, GPU and TPU for Python based-neural networks created using Tensorflow deep learning framework."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPW6CP0eYafJ"
      },
      "source": [
        "# **Selecting hardware device, dataset and neural network model**\n",
        "\n",
        "Variables \"hw_device\", \"input_dataset\", \"nn_model\" are used in this notebook for some internal decision-making in this notebook which simplifies things for students of this lab. These are not standard configuration parameters for Google Colab or TensorFlow. **Hence, make sure that the runtime in \"Runtime > Change runtime type\" is same as the value inside \"hw_device\" variable before running the notebook. You must do this manually.**\n",
        "\n",
        "Also, while generating results for different hardware devices or datasets or neural network models by changing the variables \"hw_device\" or \"input_dataset\" or \"nn_model\", **always use: \"Runtime > Restart and run all\". Otherwise it will resume training from the point where you previously stopped and that won't be correct as we want new training to start fresh after every change.**\n",
        "\n",
        "If you get the error: \"**Failed to assign a backend. No backend with GPU (or TPU) available. Would you like to use a runtime with no accelerator?**\" while using Runtime > Change runtime type<br>\n",
        "Reason: No GPU/TPU is free at the moment.<br>\n",
        "Solution: Chill out and try after sometime."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2xXMcChJyCU"
      },
      "source": [
        "# hw_device, possible options: \"CPU\", \"GPU\", \"TPU\"\n",
        "hw_device = \"GPU\"\n",
        "\n",
        "# input_dataset, possible options: \"MNIST\", \"FMNIST\", \"CIFAR10\"\n",
        "input_dataset = \"MNIST\"\n",
        "\n",
        "# nn_model, possible options: \"FC\", \"Lenet5\", \"VGG16\"\n",
        "nn_model = \"Lenet5\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSX5kZBiJc6n"
      },
      "source": [
        "# **Setting parameters related to neural network training**\n",
        "\n",
        "We now set the variables which control the neural network training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSSH_b9HmUoq"
      },
      "source": [
        "# Batch size (No. of images processed by the neural net at a time)\n",
        "batch_size = 256\n",
        "\n",
        "# Training epochs (No. of training iterations over the entire training set)\n",
        "epochs = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVb7NwIAa0mn"
      },
      "source": [
        "# **About TensorFlow and Keras**\n",
        "\n",
        "https://www.tensorflow.org/<br>\n",
        "TensorFlow is an open-source deep learning framework developed by Google. It provides many optimized building blocks such as layers, optimizers etc. to build neural network models. However, it is not very easy to use in terms of programming. <br>\n",
        "\n",
        "https://keras.io/<br>\n",
        "Keras is a high level API built on top of different frameworks as backends e.g. TensorFlow, Theano etc. It is more user-friendly to use as compared to using TensorFlow directly and describes different deep learning aspects e.g. model definition, training etc.<br>\n",
        "\n",
        "https://stackoverflow.com/questions/55178230/what-is-the-difference-between-keras-and-tf-keras<br>\n",
        "tf.keras configures Keras API to use TensorFlow backend and thus allows usage of Tensorflow specific features e.g. tf.data.Dataset as input objects. From Tensorflow 2.0, tf.keras is the default and highly recommended to start working using tf.keras."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QCCikzrkpN8"
      },
      "source": [
        "# **Importing necessary libraries**\n",
        "1.   tensroflow :\n",
        "<br> Exaplained above.\n",
        "2.   pandas:\n",
        "<br> https://pandas.pydata.org/<br>\n",
        "It is a data analysis and manipulation library.<br>\n",
        "We will use its dataframe feature (2D tabular data) later.\n",
        "3. numpy:\n",
        "<br>https://numpy.org/<br>\n",
        "It is a fast C-based library that offers many useful optimized functions for scientific computing.<br>\n",
        "We use it to preprocess our datasets.\n",
        "4. matplotlib:<br>\n",
        "https://matplotlib.org/<br>\n",
        "Matplotlib is a library for creating static, animated, and interactive visualizations in Python. <br> We use its pyplot module to visualize the input images before training and the predictions after the training.<br>\n",
        "https://stackoverflow.com/questions/43027980/purpose-of-matplotlib-inline<br>\n",
        "With %matplotlib inline, output of plotting commands is displayed in the Jupyter notebook directly below the code cell that produced it. The resulting plots will then also be stored in the notebook document.\n",
        "\n",
        "**Note:** The parts of the code labelled as \"reproducibility settings\" need not always be a part of your deep learning code. <br>Interested students can refer to the following links for more insights:<br>\n",
        "https://suneeta-mall.github.io/2019/12/22/Reproducible-ml-tensorflow.html <br>\n",
        "https://towardsdatascience.com/reproducible-models-with-weights-biases-415776c4cbb7 <br>\n",
        "https://stackoverflow.com/questions/58433555/why-are-my-results-still-not-reproducible <br>\n",
        "https://stackoverflow.com/questions/60041860/reproduce-same-results-on-each-run-keras-google-colab <br>\n",
        "https://stackoverflow.com/questions/59075244/if-keras-results-are-not-reproducible-whats-the-best-practice-for-comparing-mo/59075958\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y tensorflow keras\n",
        "\n",
        "!pip install --upgrade tensorflow==2.14.0 keras==2.14.0\n",
        "!pip install --upgrade larq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSkLksyPjsY6",
        "outputId": "69f8aea8-c1e3-474d-fa5f-bc75b89fae1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: tensorflow 2.14.0\n",
            "Uninstalling tensorflow-2.14.0:\n",
            "  Successfully uninstalled tensorflow-2.14.0\n",
            "Found existing installation: keras 2.14.0\n",
            "Uninstalling keras-2.14.0:\n",
            "  Successfully uninstalled keras-2.14.0\n",
            "Collecting tensorflow==2.14.0\n",
            "  Using cached tensorflow-2.14.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Collecting keras==2.14.0\n",
            "  Using cached keras-2.14.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (3.13.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes==0.2.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (0.2.0)\n",
            "Requirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (1.26.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (4.25.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (4.13.2)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.15,>=2.14 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (2.14.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (2.14.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow==2.14.0) (0.45.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.0) (2.38.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.0) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.0) (3.8)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.0) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.0) (3.1.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (4.9.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (2.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (2025.4.26)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (3.2.2)\n",
            "Using cached tensorflow-2.14.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (489.9 MB)\n",
            "Using cached keras-2.14.0-py3-none-any.whl (1.7 MB)\n",
            "Installing collected packages: keras, tensorflow\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-decision-forests 1.11.0 requires tensorflow==2.18.0, but you have tensorflow 2.14.0 which is incompatible.\n",
            "tensorflow-text 2.18.1 requires tensorflow<2.19,>=2.18.0, but you have tensorflow 2.14.0 which is incompatible.\n",
            "tf-keras 2.18.0 requires tensorflow<2.19,>=2.18, but you have tensorflow 2.14.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed keras-2.14.0 tensorflow-2.14.0\n",
            "Requirement already satisfied: larq in /usr/local/lib/python3.11/dist-packages (0.13.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.15.4 in /usr/local/lib/python3.11/dist-packages (from larq) (1.26.4)\n",
            "Requirement already satisfied: terminaltables>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from larq) (3.1.10)\n",
            "Requirement already satisfied: packaging>=19.2 in /usr/local/lib/python3.11/dist-packages (from larq) (24.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ydA62qWSgTM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2180b17d-a9d5-4e46-ebfd-baf200142c84"
      },
      "source": [
        "#===== Reproducibility Settings (Before TensorFlow Import) =====#\n",
        "import os\n",
        "#*IMPORANT*: Have to do this line *before* importing tensorflow\n",
        "os.environ['PYTHONHASHSEED']=str(1)\n",
        "# Set GPU execution to be deterministic\n",
        "# os.environ['TF_CUDNN_DETERMINISTIC']='true'\n",
        "# os.environ['TF_DETERMINISTIC_OPS']='true'\n",
        "#===============================================================#\n",
        "\n",
        "\n",
        "# Magic function to switch between tensorflow versions 1.x and 2.x\n",
        "%tensorflow_version 2.x\n",
        "# Import the libraries discussed in the text above.\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import larq as lq\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "# Check the tensorflow version: should be 2.x\n",
        "print(\"Tensorflow version \" + tf.__version__)\n",
        "\n",
        "\n",
        "#===== Reproducibility Settings (After TensorFlow Import) =====#\n",
        "# Import initializers to set initial weights for neural network layers.\n",
        "from tensorflow.keras import initializers\n",
        "# Limit tensorflow multithreading to a single thread.\n",
        "# tf.config.threading.set_inter_op_parallelism_threads(1)\n",
        "# tf.config.threading.set_intra_op_parallelism_threads(1)\n",
        "import random\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "def reset_random_seeds():\n",
        "   #os.environ['PYTHONHASHSEED']=str(1)\n",
        "   tf.random.set_seed(1)\n",
        "   np.random.seed(1)\n",
        "   random.seed(1)\n",
        "\n",
        "def reset_graph(reset_graph_with_backend=None):\n",
        "    if reset_graph_with_backend is not None:\n",
        "        K = reset_graph_with_backend\n",
        "        K.clear_session()\n",
        "        tf.compat.v1.reset_default_graph()\n",
        "        print(\"KERAS AND TENSORFLOW GRAPHS RESET\")\n",
        "\n",
        "reset_random_seeds()\n",
        "reset_graph(K)\n",
        "# K.set_floatx('float32')\n",
        "#==============================================================#"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n",
            "Tensorflow version 2.14.0\n",
            "KERAS AND TENSORFLOW GRAPHS RESET\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KIGLRYlSGny"
      },
      "source": [
        "# **Setting Up TensorBoard**\n",
        "https://medium.com/ydata-ai/how-to-use-tensorflow-callbacks-f54f9bb6db25\n",
        "<br>TensorBoard is a toolkit from TensorFlow which creates visualizations for internal states of the neural network model.<br>TensorBoard callback is a set of functions which are applied to record these internal states in a specified log directory during training. These logs are turned into visulaizations by invoking Tensorboard. <br>If TensorBoard is invoked before the training, the visualization is created parallely with data logging (dynamic plot). For TensorBoard invocation after training, the visualization is a static plot.\n",
        "\n",
        "https://sarbashis.github.io/installation/2019/How-to-configure-tensorboard-jupyter-inline/ <br>\n",
        "There are many ways you can call the tensorbords.\n",
        "1. Calling the tensorboard local server and open in the browser. (Most common way)\n",
        "2. Using \"%load_ext tensorboard\" which enables the TensorBoard visualization creation within the jupyter notebook. (Used in this lab.)\n",
        "\n",
        "Different parameters used in TensorBoard callback are described here: https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/TensorBoard.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XauMUGriSvBP"
      },
      "source": [
        "## CREATING AND SETTING UP TENSORBOARD\n",
        "\n",
        "# To use tensorboard within a Jupyter Notebook or Google’s Colab.\n",
        "%load_ext tensorboard\n",
        "\n",
        "# Import TensorBoard callback\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "\n",
        "# Remove old logs\n",
        "!rm -rf logs\n",
        "\n",
        "# Include date and time in the log folder title to keep logs for different runs separate.\n",
        "import datetime\n",
        "log_folder = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "# Setup the callback configuration\n",
        "callbacks = [TensorBoard(log_dir=log_folder, histogram_freq=1,write_graph=True,\n",
        "            write_images=True, update_freq='epoch', profile_batch=2, embeddings_freq=1)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HP_iFdjKqDdk"
      },
      "source": [
        "# **Setting Up Colab's CPU/GPU/TPU for usage**\n",
        "\n",
        "https://www.tensorflow.org/guide/distributed_training\n",
        "\n",
        "tf.distribute.Strategy is a TensorFlow API to distribute training across multiple GPUs or TPUs.\n",
        "We will later create our neural network models within the scope of the strategy which allows the training to use that strategy.\n",
        "\n",
        "CPUs: **Default distribution strategy** <br>\n",
        "Obtained using tf.distribute.get_strategy()<br>\n",
        "It provides no actual distribution and you can think of it as a \"no-op\" strategy.\n",
        "\n",
        "GPUs: **tf.distribute.MirroredStrategy** <br>\n",
        "Provides distributed training on multiple GPUs.\n",
        "\n",
        "TPUs: **tf.distribute.TPUStrategy**<br>\n",
        "Provides distributed training across multiple TPU cores. <br>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlGmRNZmLG7M"
      },
      "source": [
        "# **Additional information for Colab GPU/TPU usage**\n",
        "\n",
        "**GPU Types in Colab:**<br>\n",
        "https://research.google.com/colaboratory/faq.html#gpu-availability <br>\n",
        "The types of GPUs that are available in Colab vary over time. This is necessary for Colab to be able to provide access to these resources for free. The GPUs available in Colab often include Nvidia K80s, T4s, P4s and P100s. There is no way to choose what type of GPU you can connect to in Colab at any given time. Users who are interested in more reliable access to Colab’s fastest GPUs may try Colab Pro.<br>\n",
        "\n",
        "**TPU API Links:**<br>\n",
        "https://www.tensorflow.org/guide/distributed_training <br>\n",
        "The TPUClusterResolver instance helps locate the TPUs. In Colab, you don't need to specify any arguments to it.\n",
        "\n",
        "https://www.tensorflow.org/api_docs/python/tf/config/experimental_connect_to_cluster <br> It will connect to the given cluster and make devices on the cluster available to use.\n",
        "\n",
        "https://www.tensorflow.org/api_docs/python/tf/tpu/experimental/initialize_tpu_system <br> Initializes the TPU devices.<br>\n",
        "\n",
        "**TPU Set Up tutorials:**<br>\n",
        "https://colab.research.google.com/notebooks/tpu.ipynb <br>\n",
        "https://www.tensorflow.org/guide/tpu\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "haHEs0wml74k"
      },
      "source": [
        "if hw_device == \"TPU\":\n",
        "  # Locate the TPUs.\n",
        "  try:\n",
        "    resolver = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU cluster detection\n",
        "    print('Running on TPU ', resolver.cluster_spec().as_dict()['worker'])\n",
        "  except ValueError:\n",
        "    raise BaseException('ERROR: Not connected to a TPU runtime!')\n",
        "\n",
        "  # Connect to the given TPU cluster and make TPU devices available for use.\n",
        "  tf.config.experimental_connect_to_cluster(resolver)\n",
        "\n",
        "  # Initialize the TPU devices.\n",
        "  tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "\n",
        "  # Create the strategy.\n",
        "  strategy = tf.distribute.TPUStrategy(resolver)\n",
        "\n",
        "elif hw_device == \"GPU\":\n",
        "  # Directly create the strategy.\n",
        "  strategy = tf.distribute.MirroredStrategy()\n",
        "\n",
        "elif hw_device == \"CPU\":\n",
        "  # Directly create the strategy.\n",
        "  strategy = tf.distribute.get_strategy()\n",
        "\n",
        "else:\n",
        "  raise Exception(\"Not a valid hardware device.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hN2yY3mywbI0"
      },
      "source": [
        "# **Getting details of Colab hardware devices being used**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FP_6nLFLwQXU"
      },
      "source": [
        "# # Function to print the details of hardware devices being used\n",
        "# def detailed_info_hw_devices():\n",
        "#   print(\"-------------------------------------------\")\n",
        "#   # print(\"CPU Info\")\n",
        "#   print(\"-------------------------------------------\")\n",
        "#   # !cat /proc/cpuinfo\n",
        "#   print(\"===========================================\")\n",
        "#   if hw_device == \"GPU\":\n",
        "#     print(\"GPU Info\")\n",
        "#     print(\"-------------------------------------------\")\n",
        "#     !nvidia-smi\n",
        "#     print(\"===========================================\")\n",
        "#   if hw_device == \"TPU\":\n",
        "#     print(\"TPU Info\")\n",
        "#     print(\"-------------------------------------------\")\n",
        "#     print(tf.config.list_logical_devices('TPU'))\n",
        "#     print(\"===========================================\")\n",
        "\n",
        "# # Print the details of hardware devices being used\n",
        "# detailed_info_hw_devices()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mIxw1ZtkAJI"
      },
      "source": [
        "Another way of getting the hardware info is shown below. It is not required to use this cell, just for additional information."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gah6WAh1v3sE"
      },
      "source": [
        "## Get the details of all devices being used.\n",
        "# from tensorflow.python.client import device_lib\n",
        "# print(device_lib.list_local_devices())\n",
        "\n",
        "## Extract details of the GPUs\n",
        "# def check_gpus():\n",
        "#     gpus = [x.physical_device_desc.split(\",\")[1] for x in device_lib.list_local_devices() if x.device_type == 'GPU']\n",
        "#     if len(gpus) == 0:\n",
        "#       print('No GPU devices found')\n",
        "#     else:\n",
        "#       print(\"GPUs:\")\n",
        "#       print(gpus)\n",
        "#     print(\"-------------------------------------------\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRy07-oQyAd6"
      },
      "source": [
        "# **Preprocessing the datasets**\n",
        "TensorFlow provides built-in support for some datasets that can be loaded directly using load_data API.<br>\n",
        "For example, https://www.tensorflow.org/api_docs/python/tf/keras/datasets/mnist/load_data <br>\n",
        "It returns tuple of Numpy arrays for training and test set: (x_train, y_train), (x_test, y_test).<br>\n",
        "The data in both of these sets will be of uint8 datatype.\n",
        "\n",
        "https://towardsdatascience.com/guide-to-coding-a-custom-convolutional-neural-network-in-tensorflow-bec694e36ad3 <br>\n",
        "Tensrflow model will expect the input shape to be [batch_size, height, width, color_channels]. Since the images are grayscale (single-channel) for example in MNIST and Fashion MNIST, they have shape [60000,28,28] so we need to add a dummy color channel dimension to make the shape [60000,28,28,1]."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yST-Qsd4a4o6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9c05722-880e-4840-eefc-d90666b1ab14"
      },
      "source": [
        "if input_dataset == \"MNIST\":\n",
        "  (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "  # actual names of the classes\n",
        "  txt_labels = ['0','1','2','3','4','5','6','7','8','9']\n",
        "  out_classes = 10\n",
        "  # add dummy color channel dimension as image is grayscale\n",
        "  x_train = x_train / 255.0\n",
        "  x_test = x_test / 255.0\n",
        "  # Scale to [-1, 1]\n",
        "  x_train = x_train * 2.0 - 1.0\n",
        "  x_test = x_test * 2.0 - 1.0\n",
        "  # Binarize with tf.sign\n",
        "  x_train_bin = tf.sign(x_train)\n",
        "  x_test_bin = tf.sign(x_test)\n",
        "  # Replace zeros (if any) with +1\n",
        "  x_train_bin = tf.where(tf.equal(x_train_bin, 0), tf.ones_like(x_train_bin), x_train_bin)\n",
        "  x_test_bin = tf.where(tf.equal(x_test_bin, 0), tf.ones_like(x_test_bin), x_test_bin)\n",
        "  # Convert back to numpy arrays if needed\n",
        "  x_train_bin = x_train_bin.numpy()\n",
        "  x_test_bin = x_test_bin.numpy()\n",
        "  x_train_bin = np.expand_dims(x_train_bin, axis=-1)  # shape (60000, 28, 28, 1)\n",
        "  x_test_bin = np.expand_dims(x_test_bin, axis=-1)    # shape (10000, 28, 28, 1)\n",
        "  # Pad to (N, 32, 32, 1) with constant = -1 (background)\n",
        "  pad_width = ((0, 0),  # no pad on batch\n",
        "               (2, 2),  # pad 2 pixels top/bottom\n",
        "               (2, 2),  # pad 2 pixels left/right\n",
        "               (0, 0))  # no pad on channel\n",
        "  x_train_bin = np.pad(x_train_bin,\n",
        "                         pad_width,\n",
        "                         mode='constant',\n",
        "                         constant_values=-1)\n",
        "  x_test_bin  = np.pad(x_test_bin,\n",
        "                         pad_width,\n",
        "                         mode='constant',\n",
        "                         constant_values=-1)\n",
        "  print(x_train_bin.min(), x_train_bin.max())  # should print -1 and 1\n",
        "  print(x_train_bin.shape)\n",
        "  print(x_train.shape)\n",
        "\n",
        "\n",
        "else:\n",
        "    raise Exception(\"Not a valid dataset.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-1.0 1.0\n",
            "(60000, 32, 32, 1)\n",
            "(60000, 28, 28)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOKuz6CcTysG"
      },
      "source": [
        "# **Dataset distribution statistics and visualization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EK_AnMwXT5nU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b64fcfd-7ff0-4637-ebaa-d1c48df699e5"
      },
      "source": [
        "# # Extract information about the dataset distribution statistics.\n",
        "print('Distribution of train and test set:')\n",
        "print('Number of training images:', x_train.shape[0])\n",
        "print('Number of test images:', x_test.shape[0])\n",
        "print('--------------------------------------------------')\n",
        "\n",
        "# print('Distribution of digits in the dataset:')\n",
        "# # Find the unique elements of an array and also return the number of times each unique item appears in array.\n",
        "# train_labels_count = np.unique(y_train, return_counts=True)\n",
        "# # Data structure with two-dimensional tabular data with labeled axes (rows and columns).\n",
        "# dataframe_train_labels = pd.DataFrame({'Label':train_labels_count[0], 'Count':train_labels_count[1]})\n",
        "# display(dataframe_train_labels)\n",
        "# print('==================================================')\n",
        "\n",
        "\n",
        "# ## Both data visualization functions below are inspired from: https://colab.research.google.com/notebooks/tpu.ipynb\n",
        "# ## The same functions display training data now and will display the network predicitions later in \"Performing inference\" section.\n",
        "# # Function to display one image with label.\n",
        "# def display_one_sample(image, title, subplot, color):\n",
        "#   plt.subplot(subplot)\n",
        "#   #plt.axis('off')\n",
        "#   plt.axis('on')\n",
        "#   ax = plt.gca()\n",
        "#   ax.axes.xaxis.set_visible(False)\n",
        "#   ax.axes.yaxis.set_visible(False)\n",
        "#   # plt.grid(True)\n",
        "#   plt.imshow(image,cmap=plt.cm.gray_r)\n",
        "#   plt.title(title, fontsize=16, color=color)\n",
        "\n",
        "# # Function to display display a batch of 9 images with labels.\n",
        "# def display_nine_samples(images, titles, nn_model_outputs=None, infer=False):\n",
        "#   subplot = 331\n",
        "#   plt.figure(figsize=(9,9))\n",
        "#   for i in range(9):\n",
        "#     if infer == True:\n",
        "#       predicted_label = txt_labels[np.argmax(nn_model_outputs[i])]\n",
        "#       predicted_probability = np.max(nn_model_outputs[i])\n",
        "#       actual = txt_labels[np.squeeze(titles[i])]\n",
        "#       img_title = 'Actual:'+ actual + '\\n' +'Prediction:' + str(predicted_label) +'\\n' +'Probability' + str(predicted_probability)\n",
        "#       color = 'black'\n",
        "#       display_one_sample(images[i], img_title, 331+i, color)\n",
        "#     else:\n",
        "#       title = txt_labels[np.squeeze(titles[i])]\n",
        "#       color = 'black'\n",
        "#       display_one_sample(images[i], title, 331+i, color)\n",
        "#   plt.tight_layout()\n",
        "#   plt.subplots_adjust(wspace=0.8, hspace=0.1)\n",
        "#   plt.show()\n",
        "\n",
        "# # Display one image\n",
        "# print('Displaying a single image')\n",
        "# image = np.squeeze(x_train_bin[44])\n",
        "# label = np.squeeze(y_train[44])\n",
        "# display_one_sample(image, txt_labels[label], 111, 'black')\n",
        "\n",
        "# # Display a batch of images\n",
        "# print('Displying a batch of 9 images')\n",
        "# images = np.squeeze(x_test[:9])\n",
        "# labels = np.squeeze(y_test[:9])\n",
        "# display_nine_samples(images, labels)\n",
        "# print('==================================================')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distribution of train and test set:\n",
            "Number of training images: 60000\n",
            "Number of test images: 10000\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "id5b5hkj1asA"
      },
      "source": [
        "# **Creating the neural network model**\n",
        "\n",
        "**Model creation API:**<br>\n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/Sequential<br>\n",
        "**Sequential** groups a stack of layers into a neural network model object.<br>\n",
        "**add** method (e.g. model.add()) is used to add a layer to our neural network model.\n",
        "Sequential connects the neural network layers in the same sequence in which they appear the code (using .add) after the line **tf.keras.Sequential()**.\n",
        "\n",
        "**Clarification about VGG16 model:**<br>\n",
        "Original VGG16 source - https://github.com/keras-team/keras-applications/blob/master/keras_applications/vgg16.py<br>\n",
        "Original VGG16 is too big for any of our datasets, so we commented some convolution layers and changed size of last two fully connected layers in the original model. These changes can be seen as comments in the VGG16 model code in the below cell. BatchNorm Layers are also added by us to the original VGG16 model for better accuracy.<br>\n",
        "All these changes can be seen in the code for VGG16 model below.\n",
        "\n",
        "**kernel_initializer=initializers.glorot_uniform(seed=0)**:<br>\n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/initializers/GlorotUniform <br>\n",
        "It intializes convolution layers and fully-connected layers(also called dense layers) to same weights for each run. This provides reproducibilty and is optional. You can remove or modify the intializer later for your own use.\n",
        "\n",
        "**Training the model to GPU/TPU:**<br>\n",
        "https://colab.research.google.com/notebooks/tpu.ipynb<br>\n",
        "Creating the model in the TPU/GPU Strategy scope means we will train the model on the TPU/GPU. Apart from the strategy, there is no other setting specific to GPU/TPU and you can train a model with Keras fit/compile APIs like you would normally do.\n",
        "\n",
        "**Model Compilation API:**<br>\n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/Model#compile <br>\n",
        "**model.compile** API allows us to set the training environment e.g. which optimizer to use, which loss function to minimize, what output metric (e.g. accuracy) to evaluate etc.\n",
        "\n",
        "**Meaning of \"None\" in model summary shapes:**<br>\n",
        "https://pgaleone.eu/tensorflow/2018/07/28/understanding-tensorflow-tensors-shape-static-dynamic/<br>\n",
        "None denotes partially-known shape: in this case, we know the rank, but we have an unknown size for one or more dimension (everyone that has trained a model in batch is aware of this, when we define the input we just specify the feature vector shape, letting the batch_size dimension set to None, e.g.: (None, 28, 28, 1). Batch size is set later when we actually train the model using model.fit API.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7qkkW0Iz8pX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b37c8b7-753b-46b0-8acc-57b99f123d07"
      },
      "source": [
        "# Function that creats predefined model as per user choice stored in nn_model variable.\n",
        "def create_model():\n",
        "  # Common keyword arguments for binary layers\n",
        "  kwargs = dict(input_quantizer=\"ste_sign\",     # Quantize inputs using sign function\n",
        "              kernel_quantizer=\"ste_sign\",      # Quantize weights using sign function\n",
        "              kernel_constraint=\"weight_clip\",  # Clip weights to [-1, 1]\n",
        "              use_bias=False)                   # No bias used yers\n",
        "\n",
        "  model = tf.keras.models.Sequential([\n",
        "\n",
        "     # First binary convolutional layer\n",
        "    lq.layers.QuantConv2D(2, 4,                              # 2 filters, 4x4 kernel size\n",
        "                          kernel_quantizer=\"ste_sign\",\n",
        "                          kernel_constraint=\"weight_clip\",\n",
        "                          use_bias=False,\n",
        "                          input_shape=(32, 32, 1)),          # Input shape: 32x32 grayscale images, maybe look into 28x28\n",
        "\n",
        "     # Max pooling layer to downsample feature maps\n",
        "    tf.keras.layers.MaxPool2D(pool_size=(2, 2), strides=(2, 2)),\n",
        "     # Batch normalization to stabilize training\n",
        "    tf.keras.layers.BatchNormalization(momentum=0.999, scale=False),\n",
        "     # Flatten the 2D output to 1D for dense layer\n",
        "    tf.keras.layers.Flatten(),\n",
        "     # Binary dense (fully connected) layer with 30 units\n",
        "    lq.layers.QuantDense(30, **kwargs),\n",
        "     # Another batch normalization layer\n",
        "    tf.keras.layers.BatchNormalization(momentum=0.999, scale=False),\n",
        "     # Final activation layer for classification\n",
        "    tf.keras.layers.Activation(\"softmax\")\n",
        "\n",
        "    # lq.layers.QuantConv2D(16, 3, padding=\"same\", **kwargs, input_shape=(32, 32, 1)),\n",
        "    # tf.keras.layers.MaxPool2D(pool_size=(2, 2), strides=(2, 2)),\n",
        "    # tf.keras.layers.BatchNormalization(momentum=0.999, scale=False),\n",
        "\n",
        "    # tf.keras.layers.Flatten(),\n",
        "\n",
        "    # lq.layers.QuantDense(128, **kwargs),\n",
        "    # tf.keras.layers.BatchNormalization(momentum=0.999, scale=False),\n",
        "\n",
        "    # _ _ _ _ _ _ _\n",
        "  ])\n",
        "  '''\n",
        "  model = tf.keras.Sequential()\n",
        "  model.add(tf.keras.layers.Conv2D(filters=1, kernel_size=(5, 5), activation=tf.sign, input_shape=x_train_bin.shape[1:],kernel_initializer=initializers.glorot_uniform(seed=0)))\n",
        "  model.add(tf.keras.layers.MaxPool2D(pool_size=(2,2),strides=1))\n",
        "  #model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=(3, 3), activation='relu',kernel_initializer=initializers.glorot_uniform(seed=0)))\n",
        "  #model.add(tf.keras.layers.AveragePooling2D(pool_size=(2,2),strides=(2,2)))\n",
        "  model.add(tf.keras.layers.Flatten())\n",
        "  #model.add(tf.keras.layers.Dense(units=84, activation='relu',kernel_initializer=initializers.glorot_uniform(seed=0)))\n",
        "  model.add(tf.keras.layers.Dense(units=out_classes, activation = 'softmax',kernel_initializer=initializers.glorot_uniform(seed=0)))'''\n",
        "  return model\n",
        "\n",
        "# To run the model on CPU/GPU/TPU, create and compile the model in the scope of CPU/GPU/TPU.\n",
        "with strategy.scope():\n",
        "    model = create_model()\n",
        "    model.compile(\n",
        "      # Optimizer\n",
        "      #optimizer=tf.keras.optimizers.SGD(),\n",
        "      optimizer=tf.keras.optimizers.Adam(),\n",
        "      # Loss function to minimize\n",
        "      loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "      # List of metrics to monitor\n",
        "      metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],)\n",
        "\n",
        "# Display the summary of model layers, layer output dimensions and no. of parameters.\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " quant_conv2d_1 (QuantConv2  (None, 29, 29, 2)         32        \n",
            " D)                                                              \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPoolin  (None, 14, 14, 2)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " batch_normalization_2 (Bat  (None, 14, 14, 2)         6         \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 392)               0         \n",
            "                                                                 \n",
            " quant_dense_1 (QuantDense)  (None, 30)                11760     \n",
            "                                                                 \n",
            " batch_normalization_3 (Bat  (None, 30)                90        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 30)                0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 11888 (46.44 KB)\n",
            "Trainable params: 11824 (46.19 KB)\n",
            "Non-trainable params: 64 (256.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlgZ8SY2Zagv"
      },
      "source": [
        "# **Training the neural network**\n",
        "\n",
        "**API for training:**\n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit <br>\n",
        "The neural network model is trained using **model.fit** API.\n",
        "\n",
        "**TensorBoard visualizations for TPU:**<br>\n",
        "We have included TensorBoard visualizations with CPU and GPU in this notebook by passing TensorBoard callback to **model.fit** if **hw_device is set to CPU or GPU.** <br>\n",
        "For using tensorboard with TPU follow instructions at: https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/profiling_tpus_in_colab.ipynb#scrollTo=N6ZDpd9XzFeN <br> However, it needs you to create a Cloud Storage bucket for storing TensorBoard logs which needs creating a free trial on google cloud platform using billing information. Hence, we **do not include TensorBoard visulaizations** with TPU in this notebook and don't provide the callback to mode.fit when **hw_device is set to TPU**. Interested people can try this at their own financial risk and we are not responsible for any consequences.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKo5OIMXue8K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9a801de-866c-43ad-a623-cad6d4a07c18"
      },
      "source": [
        "# TensorFlow callback is used with CPU/GPU.\n",
        "if hw_device == \"GPU\" or hw_device == \"CPU\":\n",
        "  model.fit(\n",
        "    x_train_bin.astype(np.float32), y_train.astype(np.float32),\n",
        "    epochs= epochs,\n",
        "    batch_size = batch_size,\n",
        "    validation_data=(x_test_bin.astype(np.float32), y_test.astype(np.float32)),\n",
        "    validation_freq=epochs,shuffle=False, callbacks=callbacks)\n",
        "\n",
        "# TensorBoard callback is NOT used with TPU.\n",
        "# Note: TPU doesnt support float64, CPU and GPU do.\n",
        "if hw_device == \"TPU\":\n",
        "  model.fit(\n",
        "    x_train.astype(np.float32), y_train.astype(np.float32),\n",
        "    epochs= epochs,\n",
        "    batch_size = batch_size,\n",
        "    validation_data=(x_test.astype(np.float32), y_test.astype(np.float32)),\n",
        "    validation_freq=epochs,shuffle=False )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "235/235 [==============================] - 16s 63ms/step - loss: 2.0764 - sparse_categorical_accuracy: 0.6079\n",
            "Epoch 2/10\n",
            "235/235 [==============================] - 15s 66ms/step - loss: 1.4589 - sparse_categorical_accuracy: 0.8353\n",
            "Epoch 3/10\n",
            "235/235 [==============================] - 15s 66ms/step - loss: 1.3155 - sparse_categorical_accuracy: 0.8419\n",
            "Epoch 4/10\n",
            "235/235 [==============================] - 14s 60ms/step - loss: 1.2136 - sparse_categorical_accuracy: 0.8436\n",
            "Epoch 5/10\n",
            "235/235 [==============================] - 14s 61ms/step - loss: 1.1401 - sparse_categorical_accuracy: 0.8436\n",
            "Epoch 6/10\n",
            "235/235 [==============================] - 14s 61ms/step - loss: 1.0907 - sparse_categorical_accuracy: 0.8440\n",
            "Epoch 7/10\n",
            "235/235 [==============================] - 15s 62ms/step - loss: 1.0535 - sparse_categorical_accuracy: 0.8426\n",
            "Epoch 8/10\n",
            "235/235 [==============================] - 15s 65ms/step - loss: 1.0197 - sparse_categorical_accuracy: 0.8500\n",
            "Epoch 9/10\n",
            "235/235 [==============================] - 14s 61ms/step - loss: 0.9900 - sparse_categorical_accuracy: 0.8507\n",
            "Epoch 10/10\n",
            "235/235 [==============================] - 16s 66ms/step - loss: 0.9689 - sparse_categorical_accuracy: 0.8541 - val_loss: 0.9891 - val_sparse_categorical_accuracy: 0.7868\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgOxUJNmeUDQ"
      },
      "source": [
        "# **TensorBoard visualizations for CPU/GPU**\n",
        "\"%tensorboard\" invokes TensorBoard for GPU/CPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NV9xHv8WNsML"
      },
      "source": [
        "# # Launch TensorBoard.\n",
        "# if hw_device == \"GPU\" or hw_device == \"CPU\":\n",
        "#   %tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIoXffFmbk0k"
      },
      "source": [
        "# **Saving trained model.**\n",
        "\n",
        "**API for model saving:**<br>\n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/models/save_model <br>\n",
        "Use **model.save** to save the keras model. Keras model includes the network structure, weights and other internal variables. <br>\n",
        "https://keras.io/api/models/model_saving_apis/#saveweights-method<br>\n",
        "Use save_weights method if you want to save the model weights without the other internal variables.\n",
        "\n",
        "**File format for model saving**<br>\n",
        "https://www.geeksforgeeks.org/hdf5-files-in-python/<br>\n",
        "We stored the model in .h5 (HDF5) file. HDF5 file stands for Hierarchical Data Format 5. It is an open-source file format which comes in handy to store large amount of data. It stores data in a hierarchical structure within a single file. So if we want to quickly access a particular part of the file rather than the whole file, we can easily do that using HDF5. This functionality is not seen in normal text files and hence HDF5 is becoming seemingly popular.\n",
        "\n",
        "**include_optimizer flag:**<br>\n",
        "https://stackoverflow.com/questions/44258739/trained-and-loaded-keras-sequential-model-is-giving-different-result<br>\n",
        "Internal variables need to be restored so that the accuracy you had achieved on the test set at the end of training will be exactly same as the accuracy you obtain on the test set after loading the saved trained model for inference. This can be achived using **include_optimizer = True.**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYpy11GUgBAT"
      },
      "source": [
        "# # Create h5 file to save the full model.\n",
        "# trained_model = input_dataset + '_' + nn_model + '_trained' + '.h5'\n",
        "\n",
        "# # Save the model including internal variables.\n",
        "# model.save(trained_model,overwrite=True,include_optimizer=True)\n",
        "\n",
        "# # Create h5 file to save the weights only.\n",
        "# trained_model_weights_only = input_dataset + '_' + nn_model + '_trained'+ '_weights_only' + '.weights.h5'\n",
        "# print(trained_model_weights_only)\n",
        "\n",
        "# # Save the model weights, excluding internal variables.\n",
        "# # Note: During your mini-project, you can train the model using this notebook, export the weights from save_weights and\n",
        "# # use those weights for your RTL implementation.\n",
        "# model.save_weights(trained_model_weights_only, overwrite=True)\n",
        "\n",
        "# weights_file = 'MNIST_Lenet5_trained_weights_only.h5'\n",
        "\n",
        "# # Define the output CSV file\n",
        "# output_csv_file = 'model_weights.csv'\n",
        "\n",
        "# binary_weights = {}\n",
        "# for layer in model.layers:\n",
        "#     # Only process layers that actually have weights\n",
        "#     weights = layer.get_weights()\n",
        "#     if not weights:\n",
        "#         continue\n",
        "\n",
        "#     # For each weight tensor (e.g. kernel, bias), apply sign quantization:\n",
        "#     #   sign(w) gives -1 for w<0, +1 for w>=0\n",
        "#     bin_tensors = [np.where(w >= 0, 1, -1).astype(np.int8) for w in weights]\n",
        "#     print(bin_tensors)\n",
        "#     binary_weights[layer.name] = bin_tensors\n",
        "\n",
        "# # Now you can save `binary_weights` however you like, e.g. to CSV:\n",
        "# with open(output_csv_file, 'w') as f:\n",
        "#     f.write(\"layer,tensor_index,weight_value\\n\")\n",
        "#     for lname, tensors in binary_weights.items():\n",
        "#         for idx, t in enumerate(tensors):\n",
        "#             for val in t.flatten():\n",
        "#                 f.write(f\"{lname},{idx},{val}\\n\")\n",
        "\n",
        "\n",
        "# '''# Open the CSV file for writing\n",
        "# with open(output_csv_file, 'w') as f:\n",
        "#     # Write a header row (optional but good practice)\n",
        "#     f.write(\"Layer_Name,Tensor_Index,Weight_Value\\n\")\n",
        "\n",
        "#     # Iterate through the layers in the model\n",
        "#     for layer in model.layers:\n",
        "#         layer_name = layer.name\n",
        "#         weights = layer.get_weights()\n",
        "\n",
        "#         if weights:\n",
        "#             # Iterate through the weight tensors for the current layer\n",
        "#             for tensor_index, weight_tensor in enumerate(weights):\n",
        "#                 # Flatten the weight tensor to a 1D array\n",
        "#                 flattened_weights = weight_tensor.flatten()\n",
        "\n",
        "#                 # Write each weight value to the CSV file\n",
        "#                 for weight_value in flattened_weights:\n",
        "#                     f.write(f\"{layer_name},{tensor_index},{weight_value}\\n\")'''\n",
        "\n",
        "# print(f\"Weights converted and saved to {output_csv_file}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHyZZv9z4fjn"
      },
      "source": [
        "# **Performing inference**\n",
        "\n",
        "**Inference API:**<br>\n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/Model#predict<br>\n",
        "model.predict: Generates output predictions for given input samples.\n",
        "\n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/Model#evaluate<br>\n",
        "model.evaluate: Returns the loss value & accuracy for the model over the entire test set.<br>\n",
        "**Note: Run the next cell (below) 2-3 times to record the time for inference. Take value from last run. No need to restart and run all, just click on the play button on the cell's leftmost part.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RaEZhg5xIk_L"
      },
      "source": [
        "score = model.evaluate(x_test_bin, y_test, batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2fAgnCx66R3"
      },
      "source": [
        "# print('Displying a small batch of 9 images')\n",
        "# images = np.squeeze(x_test_bin[:9])\n",
        "# labels = np.squeeze(y_test[:9])\n",
        "# nn_outputs = model.predict(x_test_bin[:9])\n",
        "# display_nine_samples(images, labels, nn_outputs, True)\n",
        "# print('==================================================')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}